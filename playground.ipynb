{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pickle\n",
    "with open('/home/ema/dev/shocks/data/processed/AVAXBTC_1h.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "\n",
    "d[\"features\"]\n",
    "data = [el for el in d[\"features\"] if el[\"direction\"] != 0]\n",
    "non_shocks = [el for el in d[\"features\"] if el[\"direction\"] == 0]\n",
    "data.extend(non_shocks[: 2 * len(data)])\n",
    "\n",
    "binary = True\n",
    "if binary:\n",
    "    for el in data:\n",
    "         direction = el['direction']\n",
    "         el['direction'] = 1 if direction == -1 else direction\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).dropna()\n",
    "cols = [col for col in df.columns if col != 'direction']\n",
    "labels = df['direction'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], labels, test_size=0.3, random_state=42, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>returns</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>sigma</th>\n",
       "      <th>mu_0</th>\n",
       "      <th>mu_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:00:00</th>\n",
       "      <td>0.001031</td>\n",
       "      <td>10849.86</td>\n",
       "      <td>-0.014157</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>1.596212</td>\n",
       "      <td>-0.127735</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>-0.001687</td>\n",
       "      <td>-0.002152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 10:00:00</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>15932.58</td>\n",
       "      <td>-0.008054</td>\n",
       "      <td>-0.008086</td>\n",
       "      <td>1.694612</td>\n",
       "      <td>0.042426</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>-0.001583</td>\n",
       "      <td>-0.001473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 11:00:00</th>\n",
       "      <td>0.001019</td>\n",
       "      <td>13312.57</td>\n",
       "      <td>-0.003130</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>1.694619</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>-0.001672</td>\n",
       "      <td>-0.001510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 12:00:00</th>\n",
       "      <td>0.001021</td>\n",
       "      <td>5962.94</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>1.694609</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>-0.001458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 13:00:00</th>\n",
       "      <td>0.000997</td>\n",
       "      <td>23196.41</td>\n",
       "      <td>-0.024280</td>\n",
       "      <td>-0.024580</td>\n",
       "      <td>1.596759</td>\n",
       "      <td>-0.133617</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>-0.001664</td>\n",
       "      <td>-0.002150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31 19:00:00</th>\n",
       "      <td>0.002073</td>\n",
       "      <td>8011.64</td>\n",
       "      <td>-0.010029</td>\n",
       "      <td>-0.010079</td>\n",
       "      <td>1.478796</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>-0.000390</td>\n",
       "      <td>-0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31 20:00:00</th>\n",
       "      <td>0.002089</td>\n",
       "      <td>4698.69</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>1.478796</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>-0.000390</td>\n",
       "      <td>-0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31 21:00:00</th>\n",
       "      <td>0.002097</td>\n",
       "      <td>2296.89</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>1.478361</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.004480</td>\n",
       "      <td>-0.000241</td>\n",
       "      <td>-0.000211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31 22:00:00</th>\n",
       "      <td>0.002109</td>\n",
       "      <td>5709.03</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>1.479089</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>-0.000241</td>\n",
       "      <td>-0.000211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31 23:00:00</th>\n",
       "      <td>0.002141</td>\n",
       "      <td>6795.20</td>\n",
       "      <td>0.014695</td>\n",
       "      <td>0.014588</td>\n",
       "      <td>1.470149</td>\n",
       "      <td>0.030921</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>-0.000038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4119 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close    volume   returns  log_returns     alpha  \\\n",
       "open time                                                                  \n",
       "2021-10-11 09:00:00  0.001031  10849.86 -0.014157    -0.014258  1.596212   \n",
       "2021-10-11 10:00:00  0.001022  15932.58 -0.008054    -0.008086  1.694612   \n",
       "2021-10-11 11:00:00  0.001019  13312.57 -0.003130    -0.003135  1.694619   \n",
       "2021-10-11 12:00:00  0.001021   5962.94  0.002257     0.002254  1.694609   \n",
       "2021-10-11 13:00:00  0.000997  23196.41 -0.024280    -0.024580  1.596759   \n",
       "...                       ...       ...       ...          ...       ...   \n",
       "2022-03-31 19:00:00  0.002073   8011.64 -0.010029    -0.010079  1.478796   \n",
       "2022-03-31 20:00:00  0.002089   4698.69  0.007670     0.007641  1.478796   \n",
       "2022-03-31 21:00:00  0.002097   2296.89  0.003734     0.003727  1.478361   \n",
       "2022-03-31 22:00:00  0.002109   5709.03  0.006105     0.006086  1.479089   \n",
       "2022-03-31 23:00:00  0.002141   6795.20  0.014695     0.014588  1.470149   \n",
       "\n",
       "                         beta     sigma      mu_0      mu_1  \n",
       "open time                                                    \n",
       "2021-10-11 09:00:00 -0.127735  0.004948 -0.001687 -0.002152  \n",
       "2021-10-11 10:00:00  0.042426  0.004951 -0.001583 -0.001473  \n",
       "2021-10-11 11:00:00  0.062868  0.004951 -0.001672 -0.001510  \n",
       "2021-10-11 12:00:00  0.033923  0.004952 -0.001545 -0.001458  \n",
       "2021-10-11 13:00:00 -0.133617  0.004951 -0.001664 -0.002150  \n",
       "...                       ...       ...       ...       ...  \n",
       "2022-03-31 19:00:00  0.023961  0.004479 -0.000390 -0.000275  \n",
       "2022-03-31 20:00:00  0.023961  0.004479 -0.000390 -0.000275  \n",
       "2022-03-31 21:00:00  0.006235  0.004480 -0.000241 -0.000211  \n",
       "2022-03-31 22:00:00  0.006211  0.004484 -0.000241 -0.000211  \n",
       "2022-03-31 23:00:00  0.030921  0.004521 -0.000192 -0.000038  \n",
       "\n",
       "[4119 rows x 9 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.77       211\n",
      "           1       0.49      0.34      0.40       105\n",
      "\n",
      "    accuracy                           0.66       316\n",
      "   macro avg       0.60      0.58      0.59       316\n",
      "weighted avg       0.64      0.66      0.65       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = {\n",
    "   #-1: (len([el for el in labels if el == -1]) / len(labels)),\n",
    "    0: (len([el for el in labels if el == 0]) / len(labels)),\n",
    "    1: (len([el for el in labels if el == 1]) / len(labels)),\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(class_weight=class_weights)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.51      0.57       211\n",
      "           1       0.32      0.48      0.39       105\n",
      "\n",
      "    accuracy                           0.50       316\n",
      "   macro avg       0.49      0.49      0.48       316\n",
      "weighted avg       0.55      0.50      0.51       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "model = DummyClassifier(strategy=\"uniform\")\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/ema/dev/shocks/data/processed/AVAXBTC_5m.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "data3 = [el for el in d[\"features\"] if el[\"direction\"] != 0]\n",
    "non_shocks = [el for el in d[\"features\"] if el[\"direction\"] == 0]\n",
    "data3.extend(non_shocks[:len(data3)])\n",
    "\n",
    "df = pd.DataFrame.from_dict(data3).dropna()\n",
    "cols = [col for col in df.columns if col != 'direction']\n",
    "labels = df['direction'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], labels, test_size=0.3, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.05      0.09        42\n",
      "           0       0.56      0.82      0.67       106\n",
      "           1       0.50      0.41      0.45        63\n",
      "\n",
      "    accuracy                           0.55       211\n",
      "   macro avg       0.52      0.43      0.40       211\n",
      "weighted avg       0.53      0.55      0.49       211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = {\n",
    "   -1: (len([el for el in labels if el == -1]) / len(labels)),\n",
    "    0: (len([el for el in labels if el == 0]) / len(labels)),\n",
    "    1: (len([el for el in labels if el == 1]) / len(labels)),\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(class_weight=class_weights)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.21      0.33      0.25        42\n",
      "           0       0.48      0.32      0.38       106\n",
      "           1       0.28      0.32      0.30        63\n",
      "\n",
      "    accuracy                           0.32       211\n",
      "   macro avg       0.32      0.32      0.31       211\n",
      "weighted avg       0.36      0.32      0.33       211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "model = DummyClassifier(strategy=\"uniform\")\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pystable\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "data = d[\"data\"][[\"close\"]].iloc[: 1000]\n",
    "np_data = np.array(data[\"close\"].tolist())\n",
    "\n",
    "\n",
    "init_fit = {\"alpha\": 2, \"beta\": 0, \"sigma\": 1, \"mu\": 0, \"parameterization\": 1}\n",
    "dist = pystable.create(\n",
    "    init_fit[\"alpha\"],\n",
    "    init_fit[\"beta\"],\n",
    "    init_fit[\"sigma\"],\n",
    "    init_fit[\"mu\"],\n",
    "    init_fit[\"parameterization\"],\n",
    ")\n",
    "\n",
    "def fit(data, dist, params=(\"alpha\", \"beta\")):\n",
    "    for param in params:\n",
    "        data[param] = (\n",
    "            data[\"close\"]\n",
    "            .rolling(250)\n",
    "            .apply(lambda x: fit_levy(x, dist))\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "def fit_levy(log_returns, dist):\n",
    "    pystable.fit(dist, log_returns, len(log_returns))\n",
    "    return dist.contents.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 s, sys: 1.14 s, total: 46 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted = fit(data, dist, [\"alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 s, sys: 361 ms, total: 2.76 s\n",
      "Wall time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(10000):\n",
    "    init_fit = {\"alpha\": 2, \"beta\": 0, \"sigma\": 1, \"mu\": 0, \"parameterization\": 1}\n",
    "    dist = pystable.create(\n",
    "        init_fit[\"alpha\"],\n",
    "        init_fit[\"beta\"],\n",
    "        init_fit[\"sigma\"],\n",
    "        init_fit[\"mu\"],\n",
    "        init_fit[\"parameterization\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "window_view = sliding_window_view(np.array(np_data), window_shape=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.3 s, sys: 821 ms, total: 45.1 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = list(map(lambda x: fit_levy(x, dist), window_view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(processes=8)\n",
    "\n",
    "def _fit_levy(log_returns):\n",
    "    pystable.fit(dist, log_returns, len(log_returns))\n",
    "    return np.array([dist.contents.alpha, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 390 ms, sys: 122 ms, total: 512 ms\n",
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.65086957, 0.61655576],\n",
       "       [0.64665547, 0.61723967],\n",
       "       [0.64599075, 0.61733951],\n",
       "       ...,\n",
       "       [1.96201399, 0.02192405],\n",
       "       [1.96201399, 0.02192405],\n",
       "       [1.96201399, 0.02192405]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def _fit_levy(log_returns):\n",
    "    init_fit = {\"alpha\": 2, \"beta\": 0, \"sigma\": 1, \"mu\": 0, \"parameterization\": 1}\n",
    "    dist = pystable.create(\n",
    "        init_fit[\"alpha\"],\n",
    "        init_fit[\"beta\"],\n",
    "        init_fit[\"sigma\"],\n",
    "        init_fit[\"mu\"],\n",
    "        init_fit[\"parameterization\"],\n",
    "    )\n",
    "    pystable.fit(dist, log_returns, len(log_returns))\n",
    "    return np.array([dist.contents.alpha, dist.contents.beta])\n",
    "    \n",
    "with futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    res = executor.map(_fit_levy, window_view)\n",
    "    #futs = [executor.submit(fit_levy, window_view, ) for _ in range(8)]\n",
    "#list(res)\n",
    "#results = [f.result() for f in futs]\n",
    "np.array(list(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan, 0.65086957,\n",
       "       0.64665547, 0.64599075, 0.64112162, 0.64023736, 0.63626223,\n",
       "       0.63185311, 0.63031183, 0.62635314, 0.62635314, 0.62371769,\n",
       "       0.63173012, 0.62748317, 0.63028004, 0.63826505, 0.63684075,\n",
       "       0.64959468, 0.65225515, 0.65641029, 0.65190893, 0.65202062,\n",
       "       0.65456662, 0.67310286, 0.69960989, 0.69760901, 0.7018168 ,\n",
       "       0.70563922, 0.7329264 , 0.73978865, 0.61995397, 0.55403817,\n",
       "       0.56376113, 0.55340404, 0.52352276, 0.53416113, 0.5194347 ,\n",
       "       0.52194981, 0.51309965, 1.93014212, 1.93014212, 1.94586766,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.87200349,\n",
       "       1.86766626, 1.85167867, 1.8300389 , 1.81356156, 1.63078939,\n",
       "       1.58024947, 1.53440823, 1.51922328, 1.51274323, 1.49510637,\n",
       "       1.48846273, 1.44197245, 1.44500134, 1.39235512, 1.36520843,\n",
       "       1.34183867, 1.31961399, 1.30807275, 1.30818583, 1.31060245,\n",
       "       1.30783531, 1.31182274, 1.31076422, 1.28931466, 1.28238071,\n",
       "       1.28238071, 1.28510429, 1.28646561, 1.28714614, 1.28782659,\n",
       "       1.28782659, 1.28782659, 1.28782659, 1.28782659, 1.28782659,\n",
       "       1.28270985, 1.28270985, 1.27227102, 1.28060702, 1.27604166,\n",
       "       1.26801765, 1.25982749, 1.28246915, 1.40328774, 1.44061398,\n",
       "       1.44983366, 1.46260063, 1.4907974 , 1.50304779, 1.5806266 ,\n",
       "       1.58425312, 1.57869627, 1.56601426, 1.56029766, 1.53196034,\n",
       "       1.64263815, 1.59382766, 1.55003885, 1.54564342, 1.48728128,\n",
       "       1.45208467, 1.4439153 , 1.4302225 , 1.42566324, 1.37941374,\n",
       "       1.36431137, 1.36265466, 1.36265466, 1.36265466, 1.36265466,\n",
       "       1.3630519 , 1.34159941, 1.31881882, 1.32449814, 1.28025813,\n",
       "       1.2604818 , 1.27073978, 1.24976709, 1.2675272 , 1.27442936,\n",
       "       1.26948067, 1.28074596, 1.2814479 , 1.27657522, 1.28423824,\n",
       "       1.33073108, 1.32118012, 1.3346415 , 1.36174783, 1.38237272,\n",
       "       1.38998991, 1.38998991, 1.39434611, 1.41644105, 1.42771554,\n",
       "       1.45075061, 1.45896836, 1.48674927, 1.47479636, 1.5120996 ,\n",
       "       1.52581102, 1.52635234, 1.5501885 , 1.55715304, 1.55489325,\n",
       "       1.5800278 , 1.58201131, 1.61469536, 1.62726816, 1.93014212,\n",
       "       1.66240981, 1.93014212, 1.93014212, 1.93014212, 1.94586766,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.81487527, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.84700774, 1.82643491, 1.79949092, 1.80351549,\n",
       "       1.75256637, 1.77200751, 1.78087879, 1.69929561, 1.70296979,\n",
       "       1.60314399, 1.54816621, 1.58847763, 1.53883262, 1.52277354,\n",
       "       1.52287634, 1.49865494, 1.51280348, 1.53602225, 1.54177208,\n",
       "       1.53513792, 1.51819467, 1.52482006, 1.54257416, 1.53338487,\n",
       "       1.52109223, 1.50090741, 1.50366107, 1.50938845, 1.51827737,\n",
       "       1.48858842, 1.47561039, 1.47248969, 1.41252026, 1.4078184 ,\n",
       "       1.4078184 , 1.39451045, 1.38603471, 1.38391497, 1.40410967,\n",
       "       1.35783995, 1.34161562, 1.32928702, 1.31376186, 1.29724379,\n",
       "       1.31840983, 1.32521184, 1.31344576, 1.31255932, 1.31255932,\n",
       "       1.30613318, 1.30613318, 1.30613318, 1.32465612, 1.34277424,\n",
       "       1.3489862 , 1.36806484, 1.37418161, 1.38027641, 1.42450068,\n",
       "       1.51008136, 1.62093431, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.68399907, 1.66902667,\n",
       "       1.96201399, 1.96201399, 1.64510906, 1.63503742, 1.62200608,\n",
       "       1.63738396, 1.62788837, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.98591991, 1.92607236, 1.86648021, 1.76420781,\n",
       "       1.74696384, 1.64434984, 1.58013735, 1.50342385, 1.54953006,\n",
       "       1.44925639, 1.44942313, 1.43067214, 1.41754568, 1.38555414,\n",
       "       1.37935505, 1.37455895, 1.37669535, 1.38559048, 1.37648776,\n",
       "       1.37739887, 1.37735549, 1.36847265, 1.34068274, 1.31999022,\n",
       "       1.30367452, 1.30191487, 1.29583768, 1.33561946, 1.3617005 ,\n",
       "       1.35138045, 1.35915562, 1.38762719, 1.38829547, 1.40424618,\n",
       "       1.37119419, 1.36471135, 1.36396163, 1.36654761, 1.36091142,\n",
       "       1.35636108, 1.43756128, 1.47086296, 1.49881358, 1.49844186,\n",
       "       1.64305244, 1.66587438, 1.65342347, 1.61919655, 1.62906927,\n",
       "       1.6177853 , 1.62017324, 1.61051309, 1.59288559, 1.93014212,\n",
       "       1.93014212, 1.94089375, 1.94089375, 1.94089375, 1.94089375,\n",
       "       1.94460531, 1.94460531, 1.94460531, 1.94460531, 1.94460531,\n",
       "       1.94460531, 1.94460531, 1.94460531, 1.94460531, 1.94460531,\n",
       "       1.94460531, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399,\n",
       "       1.96201399, 1.96201399, 1.96201399, 1.96201399, 1.96201399])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.array(a)[:, 0]\n",
    "nans = [np.nan] * 249\n",
    "np.concatenate([nans, f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.5 ms, sys: 103 ms, total: 173 ms\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool(processes=4)\n",
    "\n",
    "def _fit_levy(log_returns):\n",
    "    init_fit = {\"alpha\": 2, \"beta\": 0, \"sigma\": 1, \"mu\": 0, \"parameterization\": 1}\n",
    "    dist = pystable.create(\n",
    "        init_fit[\"alpha\"],\n",
    "        init_fit[\"beta\"],\n",
    "        init_fit[\"sigma\"],\n",
    "        init_fit[\"mu\"],\n",
    "        init_fit[\"parameterization\"],\n",
    "    )\n",
    "    pystable.fit(dist, log_returns, len(log_returns))\n",
    "    return np.array([dist.contents.alpha, dist.contents.beta])\n",
    "\n",
    "a = pool.map(_fit_levy, window_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65086957, 0.65086957],\n",
       "       [0.64665547, 0.64665547],\n",
       "       [0.64599075, 0.64599075],\n",
       "       ...,\n",
       "       [1.96201399, 1.96201399],\n",
       "       [1.96201399, 1.96201399],\n",
       "       [1.96201399, 1.96201399]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(close    0.00151\n",
       " alpha    0.65087\n",
       " Name: 2021-10-02 17:35:00, dtype: float64,\n",
       " close    0.00151\n",
       " alpha    0.65087\n",
       " Name: 2021-10-02 17:35:00, dtype: float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted.iloc[249], data.loc[\"2021-10-02 17:35:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"alpha\"] = np.nan\n",
    "c = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "c[\"alpha\"] = [np.nan] * (window - 1) + fitted.iloc[window - 1: ][\"alpha\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(close    0.001326\n",
       " alpha    1.962014\n",
       " Name: 2021-10-05 08:05:00, dtype: float64,\n",
       " close    0.001326\n",
       " alpha    1.962014\n",
       " Name: 2021-10-05 08:05:00, dtype: float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted.iloc[-1], data.iloc[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}